{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Data Analytics II: Working with Data and Visualizations\n",
    "\n",
    "Welcome to Part 2 of the **Python for Data Analytics Workshop**!\n",
    "\n",
    "Now that you've dipped your toes into Python basics, it's time to level up. Imagine you're handed a spreadsheet with hundreds of rows of messy data—missing values, confusing columns, and unclear trends. How do you make sense of it all? That's where today's session comes in!\n",
    "\n",
    "## Brief Recap of Part 1\n",
    "\n",
    "In Part 1 of this workshop, we introduced the basics of Python, covering foundational concepts such as:\n",
    "\n",
    "1. **Variables and Data Types**: Understanding how to store and manipulate data.\n",
    "2. **Containers: Organizing Data Efficiently**:\n",
    "   - **Lists**: Storing multiple items.\n",
    "   - **Sets**: Creating unique collections of data.\n",
    "   - **Dictionaries**: Mapping keys to values for efficient lookups.\n",
    "3. **Making Decisions with Conditional Statements**:\n",
    "   - Using `if`, `else`, and `elif` to control program flow.\n",
    "4. **Automating Repetitive Tasks with Loops**:\n",
    "   - **`for` Loops**: Iterating through collections to perform repetitive tasks.\n",
    "\n",
    "These fundamentals set the foundation for today's more advanced topics in data analytics.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "In this hands-on workshop, you'll unlock the tools to:\n",
    "\n",
    "1. **Explore and Analyze Data with `pandas`**: Learn how to load, clean, and transform datasets.\n",
    "2. **Visualize Data with `matplotlib`**: Create impactful charts to uncover insights and tell compelling data stories.\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Imagine being the one in the room who can turn a chaotic dataset into clear, actionable insights. Whether you're identifying trends in sales, spotting customer preferences, or tracking performance metrics, these tools are your superpower for making data-driven decisions.\n",
    "\n",
    "By the end of this session, you'll be able to:\n",
    "\n",
    "- Handle messy and complex datasets with ease.\n",
    "- Extract actionable insights through data analysis.\n",
    "- Present your findings clearly and effectively using visualizations.\n",
    "\n",
    "This isn't just about coding-it's about solving real-world problems with data.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Pandas\n",
    "\n",
    "In this section, we introduce one of the most powerful libraries in Python for data analysis: **`pandas`**. Imagine you're dealing with data similar to a spreadsheet—rows and columns full of numbers, dates, and text—but you want to perform complex operations more efficiently. [`pandas`](https://pandas.pydata.org/) makes it easy to handle, manipulate, and analyze data using intuitive commands.\n",
    "\n",
    "We'll start by learning the basics of pandas, including how to import the library, load datasets, and explore them. Understanding these basics will allow you to comfortably dive into more advanced data cleaning and analysis later in the session.\n",
    "\n",
    "By the end of this section, you will be able to:\n",
    "\n",
    "- Understand what `pandas` is and why it is so popular for data analysis.\n",
    "- Load data from CSV files into pandas DataFrames.\n",
    "- Perform basic exploration of datasets to understand their structure and contents.\n",
    "\n",
    "### Loading and Exploring Data with Pandas\n",
    "\n",
    "To effectively work with data, the first step is to **load it into a `pandas` DataFrame**. DataFrames are one of the core structures in `pandas`, designed to make data analysis simple and intuitive. Think of a DataFrame as an advanced spreadsheet that allows you to easily manipulate rows and columns using Python.\n",
    "\n",
    "In this section, we will cover how to:\n",
    "\n",
    "1. **Load data from different sources**, such as CSV files, and bring it into `pandas`.\n",
    "2. **Inspect the data** to understand its structure, including columns, data types, and its size.\n",
    "3. **Explore the dataset** to get a better sense of its contents, identify missing values, detect anomalies, and assess what cleaning might be needed.\n",
    "\n",
    "By the end of this section, you'll have a solid foundation for loading and exploring data—essential skills for any data analyst.\n",
    "\n",
    "### Importing `pandas`\n",
    "\n",
    "Before we can start working with our data, we need to import the necessary libraries. In Python, libraries are like toolkit—they contain functions and methods that will help us complete specific tasks efficiently.\n",
    "\n",
    "In order to use Python libraries, we need to **`import`** them into our script. Let's start by importing `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common way to import `pandas` is by using the alias `pd`. This is a widely adopted convention in the Python community, allowing us to refer to `pandas` with the shorter name `pd`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Exploring Data\n",
    "\n",
    "The first step in any data analytics workflow is to load your dataset and get a basic understanding of its structure.\n",
    "\n",
    "#### Loading the CSV File\n",
    "\n",
    "We are going to work with a dataset named `\"sales_data.csv\"`. The dataset contains historical sales records from a supermarket chain. The data captures transactions from **four branches** across **three months**, providing valuable insights into customer behavior and business performance.\n",
    "\n",
    "The dataset contains the following key attributes (columns):\n",
    "\n",
    "1. **Invoice ID**: A unique identifier for each sales transaction.\n",
    "2. **Branch**: The branch where the sale occurred (A, B, C, or D).\n",
    "3. **City**: The location of the branch.\n",
    "4. **Customer Type**: Whether the customer is a \"Member\" or \"Non-member\".\n",
    "5. **Gender**: The gender of the customer.\n",
    "6. **Product Line**: The category of the purchased items (e.g., Electronics, Food & Beverages).\n",
    "7. **Unit Price**: Price per unit of the product (in CAD).\n",
    "8. **Quantity**: The number of items purchased in a transaction.\n",
    "9. **Tax**: A 5% tax applied to the total purchase amount.\n",
    "10. **Date**: The date of purchase.\n",
    "11. **Time**: The time of purchase, ranging from 10 AM to 9 PM.\n",
    "12. **Payment**: The payment method used (Cash, Credit Card, or E-Wallet).\n",
    "13. **COGS**: The total cost of the goods sold.\n",
    "14. **Rating**: A customer satisfaction score (1-10).\n",
    "\n",
    "Now, let's our first dataset and taking a closer look at what's inside!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "file_path = \"data/sales_data.csv\"\n",
    "\n",
    "sales_data = pd.read_csv(file_path)\n",
    "\n",
    "print(type(sales_data))  # sales_data is a variable of type DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `pd.read_csv()` reads the dataset from the specified CSV file and loads it into a `pandas` DataFrame.\n",
    "\n",
    "> **Note**: A DataFrame is a tabular data structure, like a spreadsheet, that allows you to explore, analyze, and manipulate your data efficiently in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting a Quick Overview of the Dataset\n",
    "\n",
    "To understand what we're working with, let's start by getting an overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the size of the dataset\n",
    "print(\"Number of rows and columns:\", sales_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`shape`**: Returns the dimensions of the data frame (rows, columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first and the last few rows of the dataset\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`head()`**: Displays the first few rows of the dataset, providing a quick glance at the data we have.\n",
    "- **`tail()`**: Displays the last few rows of the dataset, showing the most recent data entries.\n",
    "- You can specify the number of rows to display by passing an integer argument to these functions (e.g., `.head(2)` to display the first 2 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Last 5 rows of the dataset:\")\n",
    "sales_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding the Dataset's Structure\n",
    "\n",
    "Let's take a look at the structure of our dataset to understand the types of data we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the dataset's structure\n",
    "print(\"Basic Information about the Dataset:\")\n",
    "sales_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`info()`**: Gives us valuable information, such as column names, column indices, data types, and non-null counts. This helps us determine if there are missing values or if we need to adjust any data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary Statistics for Numerical Columns\n",
    "\n",
    "Finally, let's get some basic statistics for the numerical columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for numerical columns\n",
    "print(\"Summary Statistics:\")\n",
    "sales_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **`describe()`**: Provides summary statistics like mean, median, standard deviation, and quartiles for each numerical column, giving us a better understanding of our dataset's distribution and key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Explore the Dataset?\n",
    "\n",
    "Exploring the dataset is a critical first step in any data analytics workflow. It allows us to:\n",
    "1. **Understand the Data Structure**:\n",
    "   - Identify the columns available and their data types (e.g., numerical, categorical).\n",
    "   - Check for the completeness of data by looking at non-null counts.\n",
    "\n",
    "2. **Spot Potential Issues**:\n",
    "   - Detect missing or inconsistent values that need cleaning.\n",
    "   - Observe unusual patterns, such as outliers or unexpected distributions.\n",
    "\n",
    "3. **Gain Preliminary Insights**:\n",
    "   - Use summary statistics to understand key metrics like averages, totals, or ranges.\n",
    "   - Get a sense of how the data aligns with business questions, such as identifying high-performing branches or peak sales periods.\n",
    "\n",
    "By thoroughly exploring the dataset, we ensure it's ready for deeper analysis and visualization. This step lays the foundation for uncovering actionable insights and making data-driven decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Rows and Columns in a DataFrame\n",
    "\n",
    "Once our data is loaded, it's important to know how to access specific parts of the dataset. This includes accessing rows, columns, or even specific cells. Let's take a look at some examples.\n",
    "\n",
    "#### Accessing Columns\n",
    "\n",
    "To access a single column, we use the column name in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the 'Branch' column\n",
    "branches = sales_data[\"Branch\"]\n",
    "print(\"First 5 Branches:\")\n",
    "branches.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select multiple columns, we can pass a list of column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 'Branch' and 'City' columns\n",
    "branch_city = sales_data[[\"Branch\", \"City\"]]\n",
    "print(\"First 5 Rows of Branch and City Columns:\")\n",
    "branch_city.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Rows\n",
    "\n",
    "To access specific rows by their position, we use `.iloc[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the first row of the dataset\n",
    "first_row = sales_data.iloc[0]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the last three rows of the dataset\n",
    "sales_data.iloc[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access rows using index labels, we use `.loc[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access rows with index label 1\n",
    "sales_data.loc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the index labels\n",
    "print(\"Index labels:\", sales_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Specific Rows and Columns\n",
    "\n",
    "You can also use `.iloc[]` and `.loc[]` to access specific rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the last column for the first 5 rows\n",
    "# Using .iloc[row_index, column_index] for integer-based positions\n",
    "sales_data.iloc[:5, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access multiple columns for specific rows\n",
    "# Using .loc[row_label, column_label] for label-based positions\n",
    "rows = [2, 4, 6]\n",
    "columns = [\"Branch\", \"Quantity\"]\n",
    "sales_data.loc[rows, columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why This Matters\n",
    "Being able to access specific rows and columns is essential for:\n",
    "\n",
    "- Filtering the data to focus on relevant information.\n",
    "- Cleaning and organizing the dataset before applying transformations.\n",
    "- Selecting subsets for further analysis or visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Data in a DataFrame\n",
    "\n",
    "Filtering is one of the most powerful ways to work with your data in `pandas`. It allows you to select rows based on certain conditions. Let's look at how to filter data in our dataset.\n",
    "\n",
    "#### Example: Filtering Rows Based on a Condition\n",
    "\n",
    "Suppose we want to filter our dataset to include only sales made in Branch 'A'. We can do this using a condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where 'Branch' is 'A'\n",
    "condition = sales_data[\"Branch\"] == \"A\"\n",
    "# Use square brackets to filter rows based on the condition\n",
    "branch_a = sales_data[condition]\n",
    "\n",
    "# Display the first few rows of the filtered dataset\n",
    "print(\"Sales data for Branch A:\")\n",
    "branch_a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a condition `sales_data[\"Branch\"] == \"A\"` and use it to filter the rows. This returns a new DataFrame containing only the rows where the 'Branch' is 'A'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Filtering with Multiple Conditions\n",
    "\n",
    "We can also filter data based on multiple conditions using logical operators such as `&` (and) or `|` (or).\n",
    "\n",
    "For example, let's filter the dataset for Branch 'A' where the 'Rating' is greater than or equal to 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter highly rated sales in Branch A\n",
    "condition = (sales_data[\"Branch\"] == \"A\") & (sales_data[\"Rating\"] >= 7)\n",
    "filtered_data = sales_data[condition]\n",
    "\n",
    "print(\"Filtered Data (Branch A with Rating >= 7):\")\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧑‍💻 Exercise: Filtering Data\n",
    "\n",
    "Try filtering the dataset to select all rows where the 'City' is 'Calgary' or 'Edmonton' and the 'Payment' is made by 'Cash'. Display the first 3 rows of the resulting filtered DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create your filter condition\n",
    "# Option 1: Using `|` for OR condition\n",
    "condition1 = (sales_data[\"City\"] == \"Calgary\") | (sales_data[\"City\"] == \"Edmonton\")\n",
    "# Option 2: Using `isin()` for multiple values\n",
    "condition1 = sales_data[\"City\"].isin([\"Calgary\", \"Edmonton\"])\n",
    "\n",
    "condition2 = sales_data[\"Payment\"] == \"Cash\"\n",
    "\n",
    "# Step 2: Apply the filter condition to the DataFrame\n",
    "filtered_data = sales_data[condition1 & condition2]\n",
    "\n",
    "# Step 3: Display the first 3 rows of the filtered dataset\n",
    "# Option 1: Using `.iloc[]`\n",
    "filtered_data.iloc[:3]\n",
    "\n",
    "# Option 2: Using `.head()`\n",
    "filtered_data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `isin()`: Checks if a value is present in a list of values. It is particularly useful when working with categorical data and filtering for multiple specific values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preparation\n",
    "\n",
    "Real-world datasets are often messy, with missing values, duplicate records, or inconsistencies. Before diving into analysis, it's crucial to clean and prepare the data to ensure accurate and meaningful results.\n",
    "Let's look at some common approaches for handling missing values, removing outliers, and preparing the dataset.\n",
    "\n",
    "\n",
    "#### Handling Missing Values\n",
    "\n",
    "Missing values are common in real-world datasets, and can skew your analysis. Therefore, it's important to handle them appropriately.\n",
    "\n",
    "First, let's check for any missing values in our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "print(\"Missing values per column:\")\n",
    "sales_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sales_data.isna().sum()` help us understand which columns have missing values and how many are missing.\n",
    "\n",
    "1. **`isna()`**:\n",
    "   - This checks each cell in the DataFrame and returns `True` if the cell contains a missing value (e.g., `NaN`) and `False` otherwise.\n",
    "   - The result is a DataFrame of the same shape as `sales_data`, with `True` where data is missing and `False` where it is not.\n",
    "\n",
    "2. **`sum()`**:\n",
    "   - When applied to a DataFrame, `sum()` calculates the sum of `True` values (which are treated as `1`) for each column.\n",
    "   - This gives the total count of missing values for each column (axis 0).\n",
    "\n",
    "For example, if your dataset looks like this:\n",
    "\n",
    "| Invoice ID  | Branch | Rating |\n",
    "|-------------|--------|--------|\n",
    "| 765-26-6951 | A      | 7.0    |\n",
    "| 746-04-1077 | C      | NaN    |\n",
    "| 271-77-8740 | D      | 5.0    |\n",
    "\n",
    "`sales_data.isna()` would return:\n",
    "\n",
    "| Invoice ID | Branch | Rating |\n",
    "|------------|--------|--------|\n",
    "| False      | False  | False  |\n",
    "| False      | False  | True   |\n",
    "| False      | False  | False  |\n",
    "\n",
    "`sales_data.isna().sum()` would then return:\n",
    "```\n",
    "Invoice ID    0\n",
    "Branch        0\n",
    "Rating        1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 1: Dropping Rows with Missing Values\n",
    "\n",
    "One solution is to drop rows that have missing values in specific columns that are critical to our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values in 'Invoice ID' columns\n",
    "cleaned_data = sales_data.dropna(subset=[\"Invoice ID\"])\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"Missing values after dropping rows:\")\n",
    "cleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are dropping rows that have missing values in the 'Invoice ID' column, as this information is crucial for analysis. We use the `subset` parameter to specify the column to check for missing values. Other columns with missing values will not affect these rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 2: Filling Missing Values with a Specific Value\n",
    "\n",
    "Sometimes, it makes sense to fill missing values with a placeholder or default value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Gender' column with 'Not Specified'\n",
    "cleaned_data[\"Gender\"] = cleaned_data[\"Gender\"].fillna(\"Not Specified\")\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"Missing values after filling 'Gender' column:\")\n",
    "cleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we used the `fillna()` method to fill missing values in the 'Gender' column with 'Not Specified' to retain all rows. This is useful when the missing values are not critical to the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Solution 3: Filling Missing Values with a Calculated Value\n",
    "\n",
    "Another approach is to fill missing values with a calculated value, such as the mean, median, or mode of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values in the 'Rating' column with the median of the column\n",
    "rating_median = cleaned_data[\"Rating\"].median()\n",
    "print(\"Median of 'Rating' column:\", rating_median)\n",
    "\n",
    "cleaned_data[\"Rating\"] = cleaned_data[\"Rating\"].fillna(rating_median)\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"Missing values after filling 'Rating' column:\")\n",
    "cleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculating Missing Values Based on Other Columns\n",
    "\n",
    "In some cases, missing values can be calculated using other columns in the dataset. For example, calculating tax based on unit price and quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the missing values in the 'Tax 5%' column based on 'Unit price' and 'Quantity'\n",
    "missing_tax_rows = cleaned_data[\"Tax 5%\"].isna()\n",
    "cleaned_data.loc[missing_tax_rows, \"Tax 5%\"] = (\n",
    "    cleaned_data.loc[missing_tax_rows, \"Unit price\"]\n",
    "    * cleaned_data.loc[missing_tax_rows, \"Quantity\"]\n",
    "    * 0.05\n",
    ")\n",
    "\n",
    "# Check for missing values again\n",
    "print(\"Missing values after calculating 'Tax 5%':\")\n",
    "cleaned_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Anomalies\n",
    "\n",
    "Missing values are not the only issues that can affect the quality of your data. Anomalies, such as outliers, errors, or inconsistencies, can also impact your analysis. Therefore, it's important to identify and address these anomalies.\n",
    "\n",
    "One common approach is to detect outliers using summary statistics and visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display rows with non-positive 'Quantity' or 'Unit price'\n",
    "condition = (cleaned_data[\"Quantity\"] <= 0) | (cleaned_data[\"Unit price\"] <= 0)\n",
    "\n",
    "cleaned_data.loc[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows that satisfy the condition\n",
    "cleaned_data = cleaned_data.loc[~condition]  # ~ negates the condition\n",
    "\n",
    "# Check the dataset after removing rows\n",
    "cleaned_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicate Rows\n",
    "\n",
    "Duplicate rows can inflate your results and lead to inaccurate insights. Let's identify and remove any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the duplicate rows in the dataset\n",
    "# The duplicate rows have the same values in all columns\n",
    "num_duplicates = cleaned_data.duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the dataset\n",
    "cleaned_data = cleaned_data.drop_duplicates()\n",
    "\n",
    "cleaned_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Renaming Columns and Changing Data Types\n",
    "\n",
    "To make our dataset more readable and ensure our data is in the correct format, we can rename columns and convert data types.\n",
    "\n",
    "##### Renaming Columns\n",
    "\n",
    "Renaming columns can make our dataset easier to work with, especially if the original column names are inconsistent or unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = cleaned_data.rename(\n",
    "    columns={\n",
    "        \"Tax 5%\": \"Tax\",\n",
    "        \"Payment\": \"Payment method\",\n",
    "        \"cogs\": \"COGS\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Display the updated column names\n",
    "print(\"Updated column names:\")\n",
    "list(cleaned_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Changing Data Types\n",
    "\n",
    "It's important to ensure that each column has an appropriate data type for analysis. For example, dates should be in `datetime` format, and numerical values should have numeric types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' column to datetime format\n",
    "cleaned_data[\"Date\"] = pd.to_datetime(cleaned_data[\"Date\"])\n",
    "\n",
    "# Convert 'Quantity' and 'Rating' columns to integers\n",
    "cleaned_data[\"Quantity\"] = cleaned_data[\"Quantity\"].astype(\"int\")\n",
    "cleaned_data[\"Rating\"] = cleaned_data[\"Rating\"].astype(\"int\")\n",
    "\n",
    "# Round the 'Tax' column to 2 decimal places\n",
    "cleaned_data[\"Tax\"] = cleaned_data[\"Tax\"].round(2)\n",
    "\n",
    "# Display the data types of each column\n",
    "print(\"Data types after conversion:\")\n",
    "print(cleaned_data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finalizing the Cleaned Dataset\n",
    "\n",
    "After all the cleaning steps, we can replace the original dataset with the cleaned version for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now replace the original dataset with the cleaned one\n",
    "sales_data = cleaned_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our dataset is clean and ready for further analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Data Cleaning Matters\n",
    "\n",
    "Data cleaning ensures:\n",
    "\n",
    "- **Accuracy:** Reduces errors and inconsistencies that can skew analysis.\n",
    "- **Reliability:** Prepares the dataset for advanced analysis and modeling.\n",
    "- **Efficiency:** Saves time by resolving issues upfront.\n",
    "\n",
    "By cleaning the data, we can confidently proceed to explore trends, relationships, and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching the Dataset\n",
    "\n",
    "To gain deeper insights, you can create new columns based on existing ones. These derived columns often represent key metrics that are crucial for analysis, such as total invoice payment or profit margins.\n",
    "\n",
    "Let's enrich our dataset by adding new calculated columns can help us better understand the data and derive valuable metrics that are essential for business decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total sales amount\n",
    "sales_data[\"Total\"] = sales_data[\"Unit price\"] * sales_data[\"Quantity\"] + sales_data[\"Tax\"]\n",
    "\n",
    "# Calculate the gross profit\n",
    "sales_data[\"Gross profit\"] = sales_data[\"Total\"] - sales_data[\"COGS\"]\n",
    "\n",
    "# Calculate the profit margin as a percentage\n",
    "sales_data[\"Profit margin\"] = (sales_data[\"Gross profit\"] / sales_data[\"Total\"]) * 100\n",
    "\n",
    "# Round the calculated columns for better readability\n",
    "sales_data[[\"Total\", \"Gross profit\", \"Profit margin\"]] = sales_data[\n",
    "    [\"Total\", \"Gross profit\", \"Profit margin\"]\n",
    "].round(2)\n",
    "\n",
    "# Display the first few rows of the enriched dataset\n",
    "print(\"Enriched dataset:\")\n",
    "\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` allows us to create new columns by performing calculations directly on existing columns.\n",
    "\n",
    "The calculated columns in the above example provide valuable insights:\n",
    "\n",
    "- Total: Represents the total sales amount, including unit price, quantity, and tax.\n",
    "- Gross profit: Measures the difference between total revenue and cost of goods sold (COGS).\n",
    "- Profit margin: Shows the profitability as a percentage, which is crucial for understanding how well the sales are contributing to profit.\n",
    "\n",
    "Enriching the dataset with these calculated columns helps us gain deeper insights and allows us to answer business questions more effectively. The added metrics can also be used to generate meaningful charts or reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Cleaned and Enriched Dataset\n",
    "\n",
    "Once our data is cleaned, enriched, and ready for further analysis, it's a good practice to save it for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and enriched sales_data dataframe to a CSV file\n",
    "output_file_path = \"data/sales_data_processed.csv\"\n",
    "sales_data.to_csv(output_file_path, index=False)  # No need to write the index column\n",
    "\n",
    "print(\"DataFrame saved to\", output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By saving the cleaned and enriched dataset to a file, we can easily load it again for further analysis, reporting, or use in other projects.\n",
    "\n",
    "In the next section, we'll use `pandas`' powerful **`groupby`** functionality to summarize and aggregate our data. This will help us gain insights into sales trends, customer behavior, and branch performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis and Aggregation\n",
    "\n",
    "Data analysis often involves summarizing and extracting meaningful insights from your data. In this section, we'll learn how to **group data** and apply different **aggregation functions** to analyze trends and patterns in our dataset. This process helps us answer key business questions, such as:\n",
    "\n",
    "- How do sales vary across branches?\n",
    "- Which product lines are the most profitable?\n",
    "- What's the average customer rating for each customer type?\n",
    "- How do sales change over time?\n",
    "\n",
    "#### Grouping and Aggregating Data\n",
    "\n",
    "Grouping data is a powerful way to segment your dataset and perform aggregate calculations. The `groupby()` function helps us divide the data into groups based on one or more columns, making it easier to apply aggregate functions to each group.\n",
    "\n",
    "Common aggregation functions include:\n",
    "- **`sum()`**: Calculates the total for each group.\n",
    "- **`mean()`**: Calculates the average value for each group.\n",
    "- **`min()`** and **`max()`**: Find the minimum and maximum values for each group.\n",
    "- **`count()`**: Counts the number of entries in each group.\n",
    "\n",
    "Let's look at some practical examples of using `groupby()` and aggregation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practical Examples of Aggregation\n",
    "\n",
    "##### Example: Total Sales by Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the total sales over all data points using the `sum()` function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sales = sales_data[\"Total\"].sum()\n",
    "\n",
    "print(\"Total Sales Amount:\", total_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if we want to calculate the total sales for each branch separately? This is where `groupby()` becomes very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Sales by Branch\n",
    "branch_sales = sales_data.groupby(\"Branch\")[\"Total\"].sum()\n",
    "print(\"Total Sales by Branch:\")\n",
    "branch_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby('Branch')[\"Total\"]` term groups the data by the unique values in the 'Branch' column. For each branch, we calculate the sum of the 'Total' column to determine total sales. This helps us understand which branch generates the most revenue.\n",
    "\n",
    "##### Example: Average Gross Profit by Product Line\n",
    "\n",
    "We can calculate the average gross profit for each product line to see which product lines are the most profitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_profit = sales_data.groupby(\"Product line\")[\"Gross profit\"].mean().round(2)\n",
    "print(\"Average Profit by Product Line:\")\n",
    "product_profit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby(\"Product line\")[\"Gross profit\"].mean()` term groups the data by product line and calculates the average gross profit for each. This helps us identify which product lines contribute the most to our profits.\n",
    "\n",
    "##### Example: Average Rating and Total Payment by Customer Type\n",
    "\n",
    "We can analyze the average customer rating and total payment for each customer type (e.g., 'Member' vs. 'Normal') to understand differences in customer satisfaction and spending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_rating = sales_data.groupby(\"Customer type\")[[\"Total\", \"Rating\"]].mean()\n",
    "print(\"Average Rating by Customer Type:\")\n",
    "customer_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby('Customer type')[[\"Total\", \"Rating\"]].mean()` term groups the data by customer type and calculates the average total sales and rating for each group.\n",
    "\n",
    "##### Example: Total Quantity Sold in Each City, in Each Branch\n",
    "\n",
    "In this example, we calculate the total quantity sold in each city, grouped by branch. This will help us understand which cities have the highest sales volume in each of their branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_branch_quantity = sales_data.groupby([\"City\", \"Branch\"])[\"Quantity\"].sum()\n",
    "\n",
    "print(\"Total Quantity Sold by City and Branch:\")\n",
    "city_branch_quantity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sales_data.groupby([\"City\", \"Branch\"])[\"Quantity\"].sum()` term groups the data by city first and then branch, calculating the total quantity sold in each city for each branch.\n",
    "\n",
    "##### Example: Multiple Aggregations - Maximum Sales and Average Rating per Branch\n",
    "\n",
    "We can also apply multiple aggregations at the same time using the `agg()` method. For example, let's find the maximum sales and average rating for each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum Sales and Average Rating per Branch\n",
    "branch_summary = sales_data.groupby(\"Branch\").agg({\"Total\": \"sum\", \"Rating\": \"mean\"})\n",
    "print(\"Branch Maximum Sales and Average Rating:\")\n",
    "print(branch_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🧑‍💻 Exercise\n",
    "\n",
    "In this exercise, we aim to identify the most preferred payment method for each gender. To achieve this, you will analyze the number of times each payment method is used by individuals of different genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Gender' and 'Payment method' and count the occurrences\n",
    "\n",
    "gender_payment_counts = sales_data.groupby([\"Gender\", \"Payment method\"])[\"Payment method\"].count()\n",
    "\n",
    "print(\"Counts of Payment Methods by Gender:\")\n",
    "gender_payment_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why Use Grouping and Aggregations?\n",
    "\n",
    "- **Spot Trends**: Identify patterns in sales, profits, and customer satisfaction across categories.\n",
    "- **Data Summarization**: Convert large datasets into meaningful summaries for decision-making.\n",
    "- **Actionable Insights**: Help answer key business questions like:\n",
    "    - Which branches or products are the most successful?\n",
    "    - Are member customers more satisfied than non-members?\n",
    "\n",
    "#### Next Steps\n",
    "\n",
    "With our data cleaned, enriched, and summarized, the next step is to bring the insights to life through **visualization**. In the final section of this workshop, we'll explore how to create impactful charts using `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization: Bringing Data to Life\n",
    "\n",
    "Data visualization is the bridge between raw numbers and actionable insights. It helps us present insights in an easy-to-understand and visually appealing way. Instead of sifting through rows and columns of data, visualizations can help us spot trends, identify patterns, and communicate findings effectively.\n",
    "\n",
    "Imagine trying to explain sales trends using only numbers and tables—it can be challenging and time-consuming. Visualization is a powerful tool that allows us to convey complex information quickly and clearly. By turning data into visual elements like charts and graphs, we can help others understand our analysis more easily. Remember, a picture is worth a thousand words!\n",
    "\n",
    "\n",
    "### Introducing `matplotlib`\n",
    "\n",
    "In this section, we will use one of the most popular Python libraries for data visualization. [`matplotlib`](https://matplotlib.org/) is a versatile library for creating basic visualizations like line charts, bar charts, and scatter plots. It is highly customizable and helps us understand the basics of plotting.\n",
    "\n",
    "We can use `matplotlib` to:\n",
    "\n",
    "- Visualize sales performance across branches and product lines.\n",
    "- Analyze customer preferences and behavior.\n",
    "- Present profit trends in a visually compelling format.\n",
    "\n",
    "Let's start creating some simple plots using `matplotlib` to visualize our sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "import matplotlib.pyplot as plt  # Use the alias 'plt' for plotting\n",
    "\n",
    "# Load the processed data\n",
    "sales_data = pd.read_csv(\"data/sales_data_processed.csv\")\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "sales_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Example: Total Sales by Product Line (Horizontal Bar Chart)\n",
    "\n",
    "One of the first questions business analysts often ask is: **Which product categories contribute the most to revenue?**\n",
    "Understanding sales distribution across product lines helps identify high-performing categories and areas for improvement. A bar chart is an excellent way to compare quantities across different categories. This will provide a clear overview of which categories drive the most revenue for the business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Aggregate total sales by product line\n",
    "product_sales = sales_data.groupby(\"Product line\")[\"Total\"].sum()\n",
    "\n",
    "# Sort the products by sales\n",
    "product_sales = product_sales.sort_values()\n",
    "\n",
    "# Convert the product sales to a DataFrame\n",
    "product_sales = product_sales.reset_index()\n",
    "\n",
    "# Create a horizontal bar plot using 'barh()'\n",
    "plt.barh(product_sales[\"Product line\"], product_sales[\"Total\"])\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"Total Sales by Product Line\")\n",
    "plt.xlabel(\"Total Sales ($)\")\n",
    "plt.ylabel(\"Product Line\")\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar chart allows us to easily compare total sales across different product lines, giving us insights into the most successful product categories.\n",
    "\n",
    "### Example: Average Rating by Branch (Bar Chart)\n",
    "\n",
    "We can create a bar chart to visualize the average rating for each branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate average rating by branch\n",
    "branch_rating = sales_data.groupby(\"Branch\")[\"Rating\"].mean()\n",
    "branch_rating = branch_rating.reset_index()  # Convert it to a DataFrame\n",
    "\n",
    "# Use 'bar()' to create a vertical bar plot\n",
    "# Set the color of the bars to orange\n",
    "plt.bar(branch_rating[\"Branch\"], branch_rating[\"Rating\"], color=\"orange\")\n",
    "\n",
    "plt.title(\"Average Rating by Branch\")\n",
    "plt.xlabel(\"Branch\")\n",
    "plt.ylabel(\"Average Rating\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization helps us understand customer satisfaction across different branches.\n",
    "\n",
    "### Example: Profit Trend of the Calgary Store Over Time (Line Chart)\n",
    "\n",
    "Line charts are effective for visualizing trends over time. Let's visualize the profit trend for our store in Calgary over the three months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data based on 'City'\n",
    "calgary_data = sales_data[sales_data[\"City\"] == \"Calgary\"]\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "calgary_data[\"Date\"] = pd.to_datetime(calgary_data[\"Date\"])\n",
    "\n",
    "# Sort the data by 'Date'\n",
    "calgary_data = calgary_data.sort_values(\"Date\")\n",
    "\n",
    "# Plot the profit trend over time\n",
    "plt.figure(figsize=(15, 6))  # Set the figure size\n",
    "\n",
    "plt.plot(calgary_data[\"Date\"], calgary_data[\"Gross profit\"])\n",
    "\n",
    "plt.title(\"Gross Profit Trend in Calgary\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Gross Profit ($)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Sales Distribution by Hour (Histogram)\n",
    "\n",
    "Histograms are great for visualizing the distribution of values. Let's create a histogram to identify peak sales hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the 'Time' column\n",
    "\n",
    "sales_data[\"Time\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the hour from the 'Time' column\n",
    "sales_data[\"Hour\"] = pd.to_datetime(sales_data[\"Time\"], format=\"%H:%M:%S\").dt.hour\n",
    "\n",
    "# Determine the number of bins\n",
    "num_bins = sales_data[\"Hour\"].max() - sales_data[\"Hour\"].min() + 1\n",
    "\n",
    "plt.hist(sales_data[\"Hour\"], bins=num_bins, color=\"green\", edgecolor=\"white\")\n",
    "\n",
    "plt.title(\"Sales Distribution by Hour\")\n",
    "plt.xlabel(\"Hour of the Day\")\n",
    "plt.ylabel(\"Number of Transactions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧑‍💻 Exercise: Total Sales by Customer Rating\n",
    "\n",
    "In this example, we will create a scatter plot to visualize the relationship between the total sales and the customer rating. This will help us understand if there is a correlation between sales performance and customer satisfaction. To create the scatter plot, you can use the `scatter()` function from `matplotlib.pyplot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Use `alpha` to adjust the transparency of the points\n",
    "plt.scatter(sales_data[\"Rating\"], sales_data[\"Total\"], color=\"purple\", alpha=0.6)\n",
    "\n",
    "plt.title(\"Total Sales Amount vs. Rating\")\n",
    "plt.xlabel(\"Rating\")\n",
    "plt.ylabel(\"Total Sales Amount ($)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Data Visualization Matters\n",
    "\n",
    "\n",
    "Visualizing data allows us to tell a compelling story. Whether it's identifying trends, comparing categories, or communicating insights to others, visualizations make the data come to life. By learning how to create basic plots with `matplotlib`, you now have the required skills for:\n",
    "\n",
    "- **Storytelling**: Present data in an engaging and compelling way.\n",
    "- **Insight Generation**: Identify trends, patterns, and outliers.\n",
    "- **Decision-Making**: Effectively communicate findings to stakeholders.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this workshop, we explored the essential tools and techniques for data analytics using Python. Here's a quick recap of what we covered:\n",
    "\n",
    "1. **`pandas`**:\n",
    "   - Explored, cleaned, and enriched datasets with powerful operations for filtering, grouping, and aggregating data.\n",
    "2. **`matplotlib`**:\n",
    "   - Visualized data to uncover patterns, trends, and relationships, making insights clear and actionable.\n",
    "\n",
    "### Key Takeaways:\n",
    "- **Data Manipulation**: Mastering tools like `pandas` helps preprocess and analyze data efficiently.\n",
    "- **Data Visualization**: Using libraries like `matplotlib`, we can tell compelling stories and present insights visually.\n",
    "- **Business Relevance**: Combining these skills enables you to extract meaningful insights from raw data, supporting better decision-making and strategic planning.\n",
    "\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "This workshop is just the beginning of your data analytics journey. As you practice and apply these techniques, consider exploring advanced topics like:\n",
    "\n",
    "- Predictive analytics using machine learning.\n",
    "- Advanced visualization techniques for more impactful storytelling.\n",
    "- Automating workflows with Python to save time and improve efficiency.\n",
    "\n",
    "Keep experimenting and stay curious! Thank you for joining, and we look forward to seeing you in future workshops!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
